{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the archive\n",
    "\n",
    "import zipfile\n",
    "\n",
    "local_zip = r\"C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\archive.zip\"\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall()\n",
    "\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 155 images of brain tumor.\n",
      "There are 98 images of normal brain.\n"
     ]
    }
   ],
   "source": [
    "#checking data file\n",
    "\n",
    "source_path=r\"C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\"\n",
    "source_path_yes = os.path.join(source_path, 'Yes')\n",
    "source_path_no = os.path.join(source_path, 'No')\n",
    "\n",
    "print(f\"There are {len(os.listdir(source_path_yes))} images of brain tumor.\")\n",
    "print(f\"There are {len(os.listdir(source_path_no))} images of normal brain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should not be seeing this since the upper directory is removed beforehand\n"
     ]
    }
   ],
   "source": [
    "#create train/valid folder\n",
    "\n",
    "root_dir=r\"C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\"\n",
    "\n",
    "def create_train_val_dirs(root_path):\n",
    "    os.makedirs(os.path.join(root_path, 'training'))\n",
    "    os.makedirs(os.path.join(f'{root_path}/training', 'Yes'))\n",
    "    os.makedirs(os.path.join(f'{root_path}/training', 'No'))\n",
    "    os.makedirs(os.path.join(root_path, 'validation'))\n",
    "    os.makedirs(os.path.join(f'{root_path}/validation', 'Yes'))\n",
    "    os.makedirs(os.path.join(f'{root_path}/validation', 'No'))\n",
    "try:\n",
    "    create_train_val_dirs(root_path=root_dir)\n",
    "except FileExistsError:\n",
    "    print(\"You should not be seeing this since the upper directory is removed beforehand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\brain_tumor_dataset\n",
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\no\n",
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\training\n",
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\validation\n",
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\yes\n",
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\brain_tumor_dataset\\no\n",
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\brain_tumor_dataset\\yes\n",
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\training\\No\n",
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\training\\Yes\n",
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\validation\\No\n",
      "C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\validation\\Yes\n"
     ]
    }
   ],
   "source": [
    "#testing create folder\n",
    "\n",
    "for rootdir, dirs, files in os.walk(root_dir):\n",
    "    for subdir in dirs:\n",
    "        print(os.path.join(rootdir, subdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling data\n",
    "\n",
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "    shuffled_source = random.sample(os.listdir(SOURCE_DIR),len(os.listdir(SOURCE_DIR)))\n",
    "    training_number=int(len(shuffled_source) * SPLIT_SIZE)\n",
    "    i=0\n",
    "    \n",
    "    for item in shuffled_source:\n",
    "        item_source = os.path.join(SOURCE_DIR, item)\n",
    "        if i < training_number:\n",
    "            item_source = os.path.join(SOURCE_DIR, item)\n",
    "            if os.path.getsize(item_source) == 0:\n",
    "                print(f'{item} is zero length, so ignoring.')\n",
    "            else: \n",
    "                copyfile(item_source, os.path.join(TRAINING_DIR, item))\n",
    "            \n",
    "        if i >= training_number:\n",
    "            item_source = os.path.join(SOURCE_DIR, item)\n",
    "            if os.path.getsize(item_source) == 0:\n",
    "                print(f'{item} is zero length, so ignoring.')\n",
    "            else: \n",
    "                copyfile(item_source, os.path.join(VALIDATION_DIR, item))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Original brain tumor's directory has 155 images\n",
      "Original normal brain's directory has 98 images\n",
      "\n",
      "There are 139 images of brain tumor for training\n",
      "There are 88 images of normal for training\n",
      "There are 16 images of brain tumor for validation\n",
      "There are 10 images of normal for validation\n"
     ]
    }
   ],
   "source": [
    "#checking data shuffled\n",
    "# Test your split_data function\n",
    "\n",
    "YES_SOURCE_DIR = r\"C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\yes\"\n",
    "NO_SOURCE_DIR = r\"C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\no\"\n",
    "\n",
    "TRAINING_DIR = r\"C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\training\"\n",
    "VALIDATION_DIR = r\"C:\\Users\\vutuan\\Desktop\\My-Projects\\BTDP\\validation\"\n",
    "\n",
    "TRAINING_YES_DIR = os.path.join(TRAINING_DIR, \"yes/\")\n",
    "VALIDATION_YES_DIR = os.path.join(VALIDATION_DIR, \"yes/\")\n",
    "\n",
    "TRAINING_NO_DIR = os.path.join(TRAINING_DIR, \"no/\")\n",
    "VALIDATION_NO_DIR = os.path.join(VALIDATION_DIR, \"no/\")\n",
    "\n",
    "# Empty directories in case you run this cell multiple times\n",
    "if len(os.listdir(TRAINING_YES_DIR)) > 0:\n",
    "    for file in os.scandir(TRAINING_YES_DIR):\n",
    "        os.remove(file.path)\n",
    "if len(os.listdir(VALIDATION_YES_DIR)) > 0:\n",
    "    for file in os.scandir(VALIDATION_YES_DIR):\n",
    "        os.remove(file.path)\n",
    "if len(os.listdir(TRAINING_NO_DIR)) > 0:\n",
    "    for file in os.scandir(TRAINING_NO_DIR):\n",
    "        os.remove(file.path)\n",
    "if len(os.listdir(VALIDATION_NO_DIR)) > 0:\n",
    "    for file in os.scandir(VALIDATION_NO_DIR):\n",
    "        os.remove(file.path)\n",
    "\n",
    "# Define proportion of images used for training\n",
    "split_size = 0.9\n",
    "\n",
    "# Run the function\n",
    "# NOTE: Messages about zero length images should be printed out\n",
    "split_data(YES_SOURCE_DIR, TRAINING_YES_DIR, VALIDATION_YES_DIR, split_size)\n",
    "split_data(NO_SOURCE_DIR, TRAINING_NO_DIR, VALIDATION_NO_DIR, split_size)\n",
    "\n",
    "# Check that the number of images matches the expected output\n",
    "\n",
    "# Your function should perform copies rather than moving images so original directories should contain unchanged images\n",
    "print(f\"\\n\\nOriginal brain tumor's directory has {len(os.listdir(YES_SOURCE_DIR))} images\")\n",
    "print(f\"Original normal brain's directory has {len(os.listdir(NO_SOURCE_DIR))} images\\n\")\n",
    "\n",
    "# Training and validation splits\n",
    "print(f\"There are {len(os.listdir(TRAINING_YES_DIR))} images of brain tumor for training\")\n",
    "print(f\"There are {len(os.listdir(TRAINING_NO_DIR))} images of normal for training\")\n",
    "print(f\"There are {len(os.listdir(VALIDATION_YES_DIR))} images of brain tumor for validation\")\n",
    "print(f\"There are {len(os.listdir(VALIDATION_NO_DIR))} images of normal for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument data & create train_valid\n",
    "\n",
    "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
    "    train_datagen=ImageDataGenerator(\n",
    "        rescale=1/255,\n",
    "        # rotation_range=30,\n",
    "        # width_shift_range=0.2,\n",
    "        # height_shift_range=0.2,\n",
    "        # shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=TRAINING_DIR,\n",
    "        batch_size=20,\n",
    "        class_mode='binary',\n",
    "        target_size=(300, 300)\n",
    "    )\n",
    "    \n",
    "    validation_datagen=ImageDataGenerator(\n",
    "        rescale=1/255,\n",
    "        # rotation_range=30,\n",
    "        # width_shift_range=0.2,\n",
    "        # height_shift_range=0.2,\n",
    "        # shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        directory=VALIDATION_DIR,\n",
    "        batch_size=10,\n",
    "        class_mode='binary',\n",
    "        target_size=(300, 300)\n",
    "    )\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 227 images belonging to 2 classes.\n",
      "Found 26 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#checking generators\n",
    "train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(30, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(30, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2), \n",
    "        tf.keras.layers.Flatten(), \n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(25, activation='relu'),\n",
    "        tf.keras.layers.Dense(50, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjust_learning_rate():\n",
    "    \n",
    "    model = create_model()\n",
    "    \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch / 4))\n",
    "    \n",
    "    # Compile the model passing in the appropriate loss\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=1e-3,momentum=0.9),\n",
    "                loss='mae',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    history = model.fit(train_generator, epochs=20, callbacks=[lr_schedule])\n",
    "    \n",
    "    return history\n",
    "\n",
    "lr_history = adjust_learning_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(lr_history.history[\"lr\"], lr_history.history[\"loss\"])\n",
    "plt.axis([1e-6, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.99 or logs.get('val_accuracy')>0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 0.6532 - accuracy: 0.6344 - val_loss: 0.7368 - val_accuracy: 0.6154\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 17s 1s/step - loss: 0.5705 - accuracy: 0.7357 - val_loss: 0.6085 - val_accuracy: 0.6538\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.5863 - accuracy: 0.6960 - val_loss: 0.6054 - val_accuracy: 0.7308\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.5596 - accuracy: 0.7489 - val_loss: 0.5700 - val_accuracy: 0.7308\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.5165 - accuracy: 0.7489 - val_loss: 0.5883 - val_accuracy: 0.6154\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.4839 - accuracy: 0.7621 - val_loss: 0.6729 - val_accuracy: 0.6538\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.5345 - accuracy: 0.7489 - val_loss: 0.6068 - val_accuracy: 0.6154\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.4560 - accuracy: 0.8106 - val_loss: 0.4930 - val_accuracy: 0.7308\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.4215 - accuracy: 0.8194 - val_loss: 0.7293 - val_accuracy: 0.6538\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 17s 1s/step - loss: 0.4716 - accuracy: 0.7665 - val_loss: 0.4703 - val_accuracy: 0.7692\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.4180 - accuracy: 0.8370 - val_loss: 0.4783 - val_accuracy: 0.7308\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.3832 - accuracy: 0.8238 - val_loss: 0.6049 - val_accuracy: 0.7308\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.3518 - accuracy: 0.8634 - val_loss: 0.5321 - val_accuracy: 0.7308\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.3472 - accuracy: 0.8678 - val_loss: 0.5553 - val_accuracy: 0.7692\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.3685 - accuracy: 0.8458 - val_loss: 0.4318 - val_accuracy: 0.6923\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.4077 - accuracy: 0.8414 - val_loss: 0.3737 - val_accuracy: 0.8462\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.3748 - accuracy: 0.8678 - val_loss: 0.5500 - val_accuracy: 0.6923\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.4167 - accuracy: 0.8238 - val_loss: 0.7165 - val_accuracy: 0.6538\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.3620 - accuracy: 0.8370 - val_loss: 0.4322 - val_accuracy: 0.8077\n",
      "Epoch 20/100\n",
      " 3/12 [======>.......................] - ETA: 7s - loss: 0.3667 - accuracy: 0.8723 "
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "callbacks = myCallback()\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
